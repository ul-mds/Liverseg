[2024-08-05 17:08:24,692][flwr][INFO] - Starting Flower simulation, config: num_rounds=15, no round_timeout
[2024-08-05 17:08:27,406][flwr][INFO] - Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 72102833972.0, 'GPU': 1.0, 'node:10.50.9.88': 1.0, 'object_store_memory': 35186928844.0, 'accelerator_type:T4': 1.0, 'CPU': 8.0}
[2024-08-05 17:08:27,406][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-08-05 17:08:27,406][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 3, 'num_gpus': 1.0}
[2024-08-05 17:08:27,420][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[2024-08-05 17:08:27,421][flwr][INFO] - [INIT]
[2024-08-05 17:08:27,421][flwr][INFO] - Requesting initial parameters from one random client
[2024-08-05 17:08:31,015][flwr][INFO] - Received initial parameters from one random client
[2024-08-05 17:08:31,016][flwr][INFO] - Evaluating initial global parameters
[2024-08-05 17:08:34,939][flwr][ERROR] - Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
[2024-08-05 17:08:34,945][flwr][ERROR] - Traceback (most recent call last):
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/flwr/simulation/app.py", line 323, in start_simulation
    hist = run_fl(
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/flwr/server/server.py", line 490, in run_fl
    hist, elapsed_time = server.fit(
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/flwr/server/server.py", line 95, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/flwr/server/strategy/fedavg.py", line 167, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/mmaia/fedliver_v2/server.py", line 32, in evaluate_fn
    loss, log = test(model, testloader, criterion, device=device)
  File "/home/mmaia/fedliver_v2/model/unet.py", line 289, in test
    output = net(input)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mmaia/fedliver_v2/model/unet.py", line 99, in forward
    e1 = self.Conv1(x)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mmaia/fedliver_v2/model/unet.py", line 35, in forward
    x = self.conv(x)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/mmaia/my_envs/liver_torch/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same

[2024-08-05 17:08:34,946][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 3, 'num_gpus': 1.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 3, 'num_gpus': 1.0}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
